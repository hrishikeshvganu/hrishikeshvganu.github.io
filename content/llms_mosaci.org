* LLM training optimization
** Data loading
** Methods/optimizer
***
[[pdf:./img/Mosaic LLMs (Part 2)_ GPT-3 quality for _$500k.pdf][Mosaic LLMs (Part 2)_ GPT-3 quality for _$500k.pdf: Page 2; Quoting: And as weâ€™ll explain below, based on the new Chinchilla scaling law,]]

[[pdf:./img/Mosaic LLMs (Part 2)_ GPT-3 quality for _$500k.pdf][Mosaic LLMs (Part 2)_ GPT-3 quality for _$500k.pdf: Page 2; Quoting: platform. Our profiling shows that a compute-optimal GPT-30B can be trained on 610B tokens for less than $500k.]]



*** 30B params, 35 days, 450K USD, 600B tokens                 :llm_training:
[[pdf:./img/Mosaic LLMs (Part 2)_ GPT-3 quality for _$500k.pdf][Mosaic LLMs (Part 2)_ GPT-3 quality for _$500k.pdf: Page 3; Quoting: Figure 2: Times and Costs to train GPT models ranging from 1.3B to 70B params. Each model is paired with a compute-optimal token budget based on â€˜Training Compute-Optimal Language Modelsâ€™. All training runs were profiled with Composer on a 256xA100-40GB cluster with 1600Gbps RoCE interconnect, using a global batch size of 2048 sequences ~= 4M tokens. * This recipe is predicted to reach the same quality as the original GPT3-175B recipe. ** This recipe is the same as Chinchilla-70B.]]


[[pdf:./img/Mosaic LLMs (Part 2)_ GPT-3 quality for _$500k.pdf][Mosaic LLMs (Part 2)_ GPT-3 quality for _$500k.pdf: Page 4; Quoting: As we demonstrated in Part 1 of our LLM series, multi-node training with the right infrastructure incurs very little overhead: on our platform, we see near linear scaling when training LLMs on up to hundreds of GPUs. That means you get your work done a lot faster, with minimal increase in total cost.]]

*** failures handled
you can see how we resumed a GPT-6.7B training run that encountered a â€˜hangâ€™ (likely a failed
communication op), and a GPT-2.7B that encountered a loss spike. In both cases, we were able to seamlessly
download and resume from the last stable checkpoint, and do so either automatically or by specifying a
checkpoint path like `load_path: s3://my-bucket/my-run/ep0-ba48000.pt`.

*** Streaming dataset implementation
[[pdf:./img/Mosaic LLMs (Part 2)_ GPT-3 quality for _$500k.pdf][Mosaic LLMs (Part 2)_ GPT-3 quality for _$500k.pdf: Page 5; Quoting: When resuming from a checkpoint deep into a training run, it can take a long time to reload the dataloader state. For example, in Metaâ€™s recent OPT-175B training logs, it says that around 30 minutes was spent waiting for the dataloader to â€œfast-forwardâ€ back to its checkpointed state. Thatâ€™s 30 minutes during which all 1024 GPUs were sitting idle, wasting resources and adding frustrating latency. To avoid these problems, weâ€™ve built our own streaming dataset implementation that features deterministic and near-instant resumption, and works seamlessly with Composer so that no fast-forwarding is needed. Itâ€™s also built from the ground up to support streaming datasets from object stores like S3, GCS, etc. with no impact on LLM training throughput. Stay tuned for an upcoming blog post with more info on streaming datasets!]]



*** Composer
[[pdf:./img/Mosaic LLMs (Part 2)_ GPT-3 quality for _$500k.pdf][Mosaic LLMs (Part 2)_ GPT-3 quality for _$500k.pdf: Page 6; Quoting: FSDP. Check out the code and try customizing it yourself! How is this possible? FSDP is a PyTorch-native implementation of the ZERO sharding strategies. ZERO shards the model + gradient + optimizer state across the whole cluster rather than replicate it on each GPU. This means that even a large 70B param model with ~ 1 TB of state only has to fit within the total cluster memory, not within each GPU. This can be accomplished with a 32xA100-40GB cluster, or a 16xA100-80GB cluster, etc. Enabling CPU offloading with Composer (work in progress!) will unlock even smaller systems. Composer + FSDP also has some sweet quality-of-life features: Checkpoints are saved as a single Pytorch `.pt` file from global device0, instead of a collection of sharded files from N different devices. This makes it easier to resume runs on different cluster sizes and share models for downstream tasks. FSDP is built by PyTorch and works seamlessly with standard mixed precision training via `torch.autocast(...)` while also supporting advanced features such as low-precision gradient communication and low-precision master weights + optimizer state. FSDP can be used with Composerâ€™s auto micro-batching engine to catch and adjust for OOM errors dynamically during training, and enable hardware-agnostic training configs. Now this is all great, but what about performance? We are happy to report that when using the MosaicML platform, we are able to train these LLMs at equal- or better- utilization to highly optimized frameworks such as Megatron-LM. We regularly see 40%+ MFU (Model FLOPS Utilization) when training with real-world settings such as a GPT-13B model on 256xA100 GPUs. This matches reports from other frameworks while using lower-tier cards (40GB vs. 80GB), using larger device counts (256 GPUs vs. 8-64 GPUs), and providing way more flexibility.]]

*** Chinchilla scaling laws                                          :ATTACH:
:PROPERTIES:
:ID:       1AC5662F-4819-4D9D-9683-1D936CFC2CB1
:END:
[[pdf:./img/Mosaic LLMs (Part 2)_ GPT-3 quality for _$500k.pdf][Mosaic LLMs (Part 2)_ GPT-3 quality for _$500k.pdf: Page 7; Quoting: Figure 6: We compare the original GPT3-175B recipe with a new compute-optimal GPT-30B recipe. We substitute 175B -> 30B parameters, and 300B -> 610B tokens. Both models have nearly equal predicted loss values based on the Chinchilla scaling law (Eq 10), but the GPT-30B recipe uses 65% less compute as the GPT3-175B recipe. Compute is measured using the standard approximation `FLOPS = 6 * N * D`.]]

*** Zero infinity. NVMe offload
[[pdf:./img/zero_infinity.pdf][zero_infinity.pdf: Page 1; Quoting: Despite the capabilities of 3D parallelism for large model training, we are now arriving at the GPU memory wall [16]. The aggregate GPU memory is simply not large enough to support the growth in model size. Even with the newest NVIDIA A100 GPUs with 80 GB of memory, 3D parallelism requires 320 GPUs just to fit a trillion- parameter model for training, and scaling to a hundred trillion parameter model of the future would require over 6K GPUs even if we assume a 5x increase in GPU memory in the next few years.]]

*** Only immediate next layer params are needed from other GPUs. Pipeline parallelism
[[pdf:./img/zero_infinity.pdf][zero_infinity.pdf: Page 3; Quoting: Thus, ZeRO-3 can keep all the model states partitioned throughout the training except for the parameters that are required by the immediate computation. Heterogeneous Training Approaches Out of several hetero-]]


*** Mixed precision training. FP32 weight updates. FP6 for gradients. :llm_training:
[[pdf:./img/zero_infinity.pdf][zero_infinity.pdf: Page 3; Quoting: Large model training is generally trained in mixed precision, where the forward and backward propagation are done in FP16 and the parameter updates in FP32 [36]. This leverages the performance acceleration of the tensor core units available on modern GPUs [37].]]

*** memory for model states
[[pdf:./img/zero_infinity.pdf][zero_infinity.pdf: Page 3; Quoting: For mixed precision training with Adam optimizer, the parameters and gradients are stored in FP16 while the optimizer states consist of FP32 momen- tum, variance, parameters, and gradients. In total, each parameter requires 20 bytes of memory. The total numbei32r of parameters in a Transformer based model primarily depends on the hidden di- mension (â„ğ‘‘) and the number of Transformer layers (ğ‘›ğ‘™). Nearly all the parameters in a Transformer block come from four linear lay- ers within each block with sizes: (â„ğ‘‘, 3â„ğ‘‘), (â„ğ‘‘, â„ğ‘‘), (â„ğ‘‘, 4â„ğ‘‘) and (4â„ğ‘‘, â„ğ‘‘), respectively. Thus, the total parameters in a Transformer based model and can be approximated as 12 Ã— ğ‘›ğ‘™ Ã— â„ğ‘‘ 2 requiring a total memory 240 Ã— ğ‘›ğ‘™ Ã— â„ğ‘‘ 2 bytes to store the model states.]]

**** TODO Why 20 bytes per parameter?                          :llm_training:
- params in GPU memory for holding: 16 bit
***** - gradients in GPU memory for holding: 16 bit. Needed for xfer to other GPUs?
- gradients: optimizer : 32 bit
- momentum: optimizer : 32 bit
- variance: optimizer : 32 bit
- params: optimizer : 32 bit
- total 5*32  bits=20 bytes
