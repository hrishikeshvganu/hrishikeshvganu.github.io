#+options: num:10
#+TITLE:  Practical Problems with LLMs
<2023-08-19 Sat>
For the last 7 months or so LLMs were on a hype cycle. My own understanding has evolved and at this point I think there are the following  practical problems.[fn:3]
** Not a silver bullet
Realization that LLMs can't solve most problems when used out-of-the-box. They need to be combined with other ML models to build a complete use case. Eg: If the idea is to identify the article type in the user query a multiclass classifier needs to be used downstream in the pipeline. If you don't do this the LLM will  predict article types that don't exist in your catalog. This is good news for ML scientists like me. Looks like LLMs won't take over our jobs anytime soon :)
** Unreasonable inference costs
When you  use commercial LLM APIs for inferencing and you have a MAU (Monthly Active User) of tens of millions, paying for inference hurts.  While commercial APIs might be charging a markup on the inferencing cost, the markup is likely to be minimal. This is because Google and Microsoft are in a race to acquire more users since this enables them to improve their models even more based on human feedback.

Even if one uses smaller custom models, the costs are typically an order of magnitude higher than the inference cost of other ML models like GBDTs or feed forward neural networks that used to be the workhorse ML algorithms uptil now.

#+CAPTION: Azure OpenAI inference pricing
[[./img/azure_openai_pricing.png]]

The LLM race threatens to have an impact on Google's variable cost of serving a query. I'm sure it has some tricks up it's sleeve-like smart caching or adaptively deciding which queries can be processed through their existing flow and which need to go to an LLM.

#+CAPTION: Incremental cost for Google depending on LLM coverage of queries
[[./img/google_cost.png]]
** Tricky to get the LLM inference settings right
Beam search is typically used for prediction/inference. There are settings like temperature, top-p, length penalty and usually you have to spend a good deal of time to get these right. This is what Bard had to say when I asked "temperature and other settings llms":
------
The best way to find the ideal settings for a specific task is to experiment. Try different settings and see what results you get. You may need to adjust the settings depending on the specific input text.
------
** Poor support for fine-tuned custom models.
The commercial APIs I've used have rudimentary support for fine-tuning. Fine tuning is not easy and when it's available it's limited. I think it's related to the fact that if the commercial API has to support a custom model they need to create a custom end-point. Creating a custom end-point needs more hardware. This is exacerbated by the huge supply crunch for GPUs.[fn:2]
* Footnotes

[fn:3]Arguably some companies are trying out fine-tuned versions of custom models which could mitigate some of these problems, but they are in a minority. Because 95% of companies use commercial APIs these problems will continue to shape the future adoption of LLMs for real use cases.

[fn:2]https://www.tomshardware.com/news/chinas-bytedance-has-gobbled-up-dollar1-billion-of-nvidia-gpus-for-ai-this-year
