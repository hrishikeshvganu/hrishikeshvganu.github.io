#+title: Building a SOTA Search Ranking System
*** Ranking errors on the first page are worse than those on later pages
In both cases in the figure, there are 2 products that are ranked incorrectly. The one on left is worse.
#+CAPTION: All ranking errors are not the same
#+ATTR_HTML: :alt ranking_diff :title Action! :align right
[[file:img/myimage.png]]

**** Capturing this property through ML is not easy
A loss like NDCG takes into account this consideration.
$$NDCG@k = \sum_{1}^{k}GAIN(i) DISCOUNT(i) $$
Note: I use x<- y  to denote that x depends on y.
- DISCOUNT depends on the rank (r).
- DISCOUNT<-Rank<- Predicted score ($S_i$)
- $S_{i}$<-$\Theta$ (parameters over which the NDCG can be optimized)
However the rank of a product is a step function of the predicted score.


*Therefore need to find a proxy that's differentiable and is an upper bound for NDCG or similar losses that matter to business*

***** How to find a proxy metric?
It's not easy since it's an inverse problem. I'll go into the theory later but I'll be practical and discuss an algorithm called LambdaRank that has been very successful in the real world. I'll provide a hand wavy sketch of how it works (not a proof. There might not be a proof so I'll focus on the emprical)
#+CAPTION: Ranking "force" heuristic of LambdaRank
#+ATTR_HTML: :alt ranking_force :title Action! :align right


[[./img/ranking_force.png]]

$$ \mathrm{Force}_{k,l} = \lambda_{k,l}  \mathrm{Scalefactor}(k,l) \Delta NDCG (k,l) $$
Where $ {Scalefactor}(k,l)$ is a scaling factor from 0 to 1. This factor is larger if the difference in scores of $k$ and $l$ is large and negative. $\Delta NDCG (k,l)$ is the change in NDCG if the product positons are swapped.
- The "force" is used as the residual (gradient of the training loss) that has to be optimized through iterations of gradient boosting.
- Thus a pair of products in mutually incorrect order gets a higher penalty if the difference in current scores is large and negative.
